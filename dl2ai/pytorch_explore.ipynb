{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0439,  0.2717],\n",
       "        [-0.2739, -0.3007],\n",
       "        [-0.0572, -0.3011],\n",
       "        [ 0.2190,  0.1893],\n",
       "        [-0.0176,  0.1996],\n",
       "        [-0.0494, -0.0703],\n",
       "        [-0.0410,  0.0712],\n",
       "        [ 0.0198,  0.2211],\n",
       "        [ 0.3262,  0.2361],\n",
       "        [ 0.1248, -0.1574]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate normal data\n",
    "torch.normal(0, 0.2, (10,2,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.1294]]), tensor([0.0873]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Slicing Tensors\n",
    "X = torch.normal(0, 1, (3,2,1)) \n",
    "X[1, 1:2,], X[0, 1,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Linear Layer\n",
    "* It takes any (n1, n2, ..., nk, in_dim) tensor and maps it to (n1, n2, ..., nk, out_dim) array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim, out_dim = 4, 2\n",
    "linear_layer = nn.Linear(in_features=in_dim, out_features=out_dim, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.6020, 0.7538, 0.4929, 0.6953],\n",
       "          [0.5153, 0.4116, 0.6765, 0.0383],\n",
       "          [0.0749, 0.8834, 0.7991, 0.8148]],\n",
       "\n",
       "         [[0.2680, 0.3155, 0.4103, 0.2258],\n",
       "          [0.4591, 0.5276, 0.2865, 0.3702],\n",
       "          [0.3893, 0.3579, 0.8499, 0.0464]]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 3, 4])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 3\n",
    "X = torch.rand(1, 2, batch_size, in_dim)\n",
    "display(X)\n",
    "display(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.8013, -0.3974],\n",
       "          [-0.5385, -0.1659],\n",
       "          [-0.7853, -0.0920]],\n",
       "\n",
       "         [[-0.3974, -0.0973],\n",
       "          [-0.5067, -0.3278],\n",
       "          [-0.5550, -0.0122]]]], grad_fn=<UnsafeViewBackward>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "Takes any tensor of shape (d1, d2, ..., dk, D) and returns a tensor of the same shape.\n",
    "Softmax is applied along the last dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(1, 2, 3, 4, 5)\n",
    "X = torch.rand(4, 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4340, 0.5660],\n",
       "         [0.2812, 0.7188],\n",
       "         [0.5198, 0.4802]],\n",
       "\n",
       "        [[0.2769, 0.7231],\n",
       "         [0.5045, 0.4955],\n",
       "         [0.5857, 0.4143]],\n",
       "\n",
       "        [[0.2973, 0.7027],\n",
       "         [0.6497, 0.3503],\n",
       "         [0.5015, 0.4985]],\n",
       "\n",
       "        [[0.6732, 0.3268],\n",
       "         [0.5544, 0.4456],\n",
       "         [0.3603, 0.6397]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.functional.softmax(X, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch.repeat_interleave\n",
    "\n",
    "* What do `torch.repeat_interleave` and `torch.tile` do? How are they different?\n",
    "    - `torch.tile` is available only pytorch version >= 1.8\n",
    "* How's it different from numpy.repeat and numpy.tile?\n",
    "* numpy.repeat ~ torch.repeat_interleave\n",
    "* toch.tile ~ torch.tile ( >= v1.8)\n",
    "* np.tile ~ torch.repeat ( <= v1.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.tensor([[1, 2], [3, 4]])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [1, 2],\n",
       "        [1, 2],\n",
       "        [3, 4],\n",
       "        [3, 4],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.repeat_interleave(y, 3, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 1, 2],\n",
       "        [3, 4, 3, 4],\n",
       "        [1, 2, 1, 2],\n",
       "        [3, 4, 3, 4]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DON'T USE\n",
    "# this is replaced by torch.tile or tensor.tile\n",
    "y.repeat((2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yNP = np.array([1,2])\n",
    "yNP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 2, 2, 2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.repeat(yNP, 3)  # also yNP.repeat(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.repeat is similar to torch.repeat_interleave\n",
    "np.repeat(yNP, [3, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 1, 2],\n",
       "       [1, 2, 1, 2]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.tile(yNP, (2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsqueeze and BMM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, M = 5, 7\n",
    "keys = torch.randn(N, M)\n",
    "values = torch.randn(N, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8522,  0.3205,  0.9211, -0.6583,  0.4858, -1.7299, -1.1148],\n",
       "        [-0.7381,  0.7560, -1.5419,  0.9316,  0.3500,  0.3181,  0.1209],\n",
       "        [-1.8701, -0.7071, -1.8924, -0.1258, -0.2905, -0.0929,  0.1500],\n",
       "        [ 1.4270,  0.5234, -0.7496,  0.6419,  0.6141,  0.2950,  0.8062],\n",
       "        [ 0.3389, -0.8555, -2.3870, -1.8503,  0.9953,  1.0388,  2.7027]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.2002e-01, -8.5105e-01, -3.6576e-01, -8.2473e-01, -7.8378e-01,\n",
       "         -9.2171e-02,  9.3114e-01],\n",
       "        [-2.4213e+00, -1.0258e+00,  1.2937e+00, -1.3050e+00,  1.4876e+00,\n",
       "         -8.6324e-02, -8.6592e-01],\n",
       "        [-1.8177e+00,  4.6732e-01, -1.6856e-01,  5.2153e-01,  6.0094e-01,\n",
       "          7.8206e-01,  8.6765e-01],\n",
       "        [ 1.3875e+00, -1.5491e-01, -9.0206e-01,  1.8910e-03, -1.8728e+00,\n",
       "          8.4011e-01,  3.3396e-01],\n",
       "        [-6.3718e-02,  1.3403e+00, -6.0417e-01,  1.9313e+00,  1.0785e+00,\n",
       "          9.9741e-01, -8.9084e-01]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to do the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.7692, -1.8104,  3.2051,  1.9434, -3.5976])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(keys*values).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 7]), torch.Size([5, 1, 7]), torch.Size([5, 7]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unsqueeze adds and additional dimension of 1\n",
    "# squeeze removes a dimension of 1\n",
    "keys.shape, keys.unsqueeze(1).shape, keys.unsqueeze(1).squeeze().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With batch multiplication there is a lot of reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.7692, -1.8104,  3.2051,  1.9434, -3.5976])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.bmm(keys.unsqueeze(1), values.unsqueeze(-1)).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With einsum it's elegant and simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.7692, -1.8104,  3.2051,  1.9434, -3.5976])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum(\"ij, ij->i\", values, keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "d2l"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
