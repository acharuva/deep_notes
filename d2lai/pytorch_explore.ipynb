{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2904, -0.2004],\n",
       "        [ 0.2562, -0.1379],\n",
       "        [-0.1193,  0.0258],\n",
       "        [-0.1380, -0.2429],\n",
       "        [-0.0552,  0.1956],\n",
       "        [-0.1482, -0.2079],\n",
       "        [-0.0026,  0.2449],\n",
       "        [-0.1600, -0.2543],\n",
       "        [-0.1913, -0.2307],\n",
       "        [ 0.1029, -0.3219]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate normal data\n",
    "torch.normal(0, 0.2, (10,2,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.2372]]), tensor([-0.1493]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Slicing Tensors\n",
    "X = torch.normal(0, 1, (3,2,1)) \n",
    "X[1, 1:2,], X[0, 1,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Linear Layer\n",
    "* It takes any (n1, n2, ..., nk, in_dim) tensor and maps it to (n1, n2, ..., nk, out_dim) array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim, out_dim = 4, 2\n",
    "linear_layer = nn.Linear(in_features=in_dim, out_features=out_dim, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.7573, 0.6799, 0.5769, 0.6115],\n",
       "          [0.4796, 0.9606, 0.2561, 0.0980],\n",
       "          [0.6114, 0.3206, 0.5559, 0.4730]],\n",
       "\n",
       "         [[0.6714, 0.1972, 0.4625, 0.1150],\n",
       "          [0.7412, 0.8133, 0.9782, 0.4173],\n",
       "          [0.3976, 0.4183, 0.0580, 0.9802]]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 3, 4])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 3\n",
    "X = torch.rand(1, 2, batch_size, in_dim)\n",
    "display(X)\n",
    "display(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.8973, -0.3517],\n",
       "          [-0.4788, -0.3368],\n",
       "          [-0.7347, -0.2338]],\n",
       "\n",
       "         [[-0.5787, -0.1360],\n",
       "          [-1.0363, -0.4824],\n",
       "          [-0.5826, -0.1959]]]], grad_fn=<UnsafeViewBackward>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "Takes any tensor of shape (d1, d2, ..., dk, D) and returns a tensor of the same shape.\n",
    "Softmax is applied along the last dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(1, 2, 3, 4, 5)\n",
    "X = torch.rand(4, 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.6898, 0.3102],\n",
       "         [0.5261, 0.4739],\n",
       "         [0.3303, 0.6697]],\n",
       "\n",
       "        [[0.6150, 0.3850],\n",
       "         [0.2907, 0.7093],\n",
       "         [0.5548, 0.4452]],\n",
       "\n",
       "        [[0.6301, 0.3699],\n",
       "         [0.4671, 0.5329],\n",
       "         [0.5185, 0.4815]],\n",
       "\n",
       "        [[0.5783, 0.4217],\n",
       "         [0.4964, 0.5036],\n",
       "         [0.6278, 0.3722]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.functional.softmax(X, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch.repeat_interleave\n",
    "\n",
    "* What do `torch.repeat_interleave` and `torch.tile` do? How are they different?\n",
    "    - `torch.tile` is available only pytorch version >= 1.8\n",
    "* How's it different from numpy.repeat and numpy.tile?\n",
    "* numpy.repeat ~ torch.repeat_interleave\n",
    "* toch.tile ~ torch.tile ( >= v1.8)\n",
    "* np.tile ~ torch.repeat ( <= v1.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.tensor([[1, 2], [3, 4]])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [1, 2],\n",
       "        [1, 2],\n",
       "        [3, 4],\n",
       "        [3, 4],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.repeat_interleave(y, 3, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 1, 2],\n",
       "        [3, 4, 3, 4],\n",
       "        [1, 2, 1, 2],\n",
       "        [3, 4, 3, 4]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DON'T USE\n",
    "# this is replaced by torch.tile or tensor.tile\n",
    "y.repeat((2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yNP = np.array([1,2])\n",
    "yNP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 2, 2, 2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.repeat(yNP, 3)  # also yNP.repeat(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.repeat is similar to torch.repeat_interleave\n",
    "np.repeat(yNP, [3, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 1, 2],\n",
       "       [1, 2, 1, 2]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.tile(yNP, (2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsqueeze and BMM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, M = 5, 7\n",
    "keys = torch.randn(N, M)\n",
    "values = torch.randn(N, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.9960,  0.7362, -0.1414, -1.0739,  0.8829,  0.9771,  0.5810],\n",
       "        [-0.7134,  0.4469,  1.0334,  0.0054,  0.1181,  1.1070,  0.7764],\n",
       "        [-0.2494, -0.5957, -1.0957,  0.3801, -1.2862, -0.9432,  0.4663],\n",
       "        [-0.6413,  1.3547, -1.7787, -0.5930,  0.7135, -1.3045,  0.7364],\n",
       "        [-0.5168,  0.2317, -0.4032, -0.0285, -1.3790,  1.0844, -2.1884]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3595, -1.5099, -0.3294, -0.1450, -0.0164, -0.1691, -0.8583],\n",
       "        [-0.0958,  1.3129, -1.4244,  0.5585,  0.3352, -1.1671,  1.2605],\n",
       "        [ 1.0333,  0.3613, -0.1452, -0.8588, -1.1601,  1.3795,  0.6896],\n",
       "        [-0.5574, -0.2654, -0.4720, -0.0266, -0.5568, -1.4078,  0.2490],\n",
       "        [-0.7238,  0.1196, -2.1268, -0.6486,  0.5666, -0.7749,  0.3147]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to do the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.3051, -1.0878, -0.1277,  2.4757, -1.0327])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(keys*values).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 7]), torch.Size([5, 1, 7]), torch.Size([5, 7]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unsqueeze adds and additional dimension of 1\n",
    "# squeeze removes a dimension of 1\n",
    "keys.shape, keys.unsqueeze(1).shape, keys.unsqueeze(1).squeeze().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With batch multiplication there is a lot of reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.3051, -1.0878, -0.1277,  2.4757, -1.0327])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.bmm(keys.unsqueeze(1), values.unsqueeze(-1)).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With einsum it's elegant and simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.3051, -1.0878, -0.1277,  2.4757, -1.0327])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum(\"ij, ij->i\", values, keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_matrices(*shapes):\n",
    "    return [torch.randn(shape) for shape in shapes]        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = rand_matrices((3, 2), (3, 1))\n",
    "torch.cat([X, Y], dim=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = rand_matrices((3,2), (2, 4))\n",
    "torch.matmul(X, Y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 1, 0]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "F.one_hot(\n",
    "    torch.tensor([0, 2, 3]), num_classes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looping over an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = rand_matrices((5, 3, 2))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2])\n",
      "torch.Size([3, 2])\n",
      "torch.Size([3, 2])\n",
      "torch.Size([3, 2])\n",
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "for x in X:\n",
    "    print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "d2l"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
